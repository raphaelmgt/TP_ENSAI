{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695bd629",
   "metadata": {},
   "source": [
    "## 1. Lecture et préparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67035c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "def import_index(chemin: str):\n",
    "    \n",
    "    with open(chemin, \"r\", encoding=\"utf-8\") as f:\n",
    "        index = json.load(f)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de653ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_index = import_index(chemin=\"input/brand_index.json\")\n",
    "description_index = import_index(chemin=\"input/description_index.json\")\n",
    "origin_index = import_index(chemin=\"input/origin_index.json\")\n",
    "origin_synonyms = import_index(chemin=\"input/origin_synonyms.json\")\n",
    "reviews_index = import_index(chemin=\"input/reviews_index.json\")\n",
    "title_index = import_index(chemin=\"input/title_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5fc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def create_token(title: str):\n",
    "    doc = nlp(title)\n",
    "\n",
    "    tokens = [\n",
    "        token.text\n",
    "        for token in doc\n",
    "        if not token.is_stop and not token.is_punct\n",
    "    ]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a7ab5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ensai/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "import nltk \n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "def find_synonyms(word: str):\n",
    "    \"\"\" \n",
    "    This fonction find the list of all synonyms of a given word\n",
    "    \"\"\"\n",
    "\n",
    "    # nlp.add_pipe(\"wordnet\", after=\"tagger\")\n",
    "\n",
    "    doc = nlp(word)\n",
    "\n",
    "    for synset in doc[0]._.wordnet.synsets():\n",
    "        print(\"Synset :\", synset.name())\n",
    "        print(\"Synonymes :\", synset.lemma_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0a29358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset : china.n.01\n",
      "Synonymes : ['China', \"People's_Republic_of_China\", 'mainland_China', 'Communist_China', 'Red_China', 'PRC', 'Cathay']\n",
      "Synset : china.n.02\n",
      "Synonymes : ['china']\n",
      "Synset : taiwan.n.01\n",
      "Synonymes : ['Taiwan', 'China', 'Nationalist_China', 'Republic_of_China']\n",
      "Synset : chinaware.n.01\n",
      "Synonymes : ['chinaware', 'china']\n"
     ]
    }
   ],
   "source": [
    "find_synonyms(\"China\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58efd66",
   "metadata": {},
   "source": [
    "## 2. Filtrage des documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cd4429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(query: str):\n",
    "\n",
    "    tokens = query.split(\" \")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a8f83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elephant a noel\n",
      "['elephant', 'a', 'noel']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_query(query: str):\n",
    "    query = query.strip().lower()\n",
    "    query = \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", query)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "    query = re.sub(r\"\\s+\", \" \", query)\n",
    "    return query\n",
    "\n",
    "print(normalize_query(\"  Éléphant   à Noël  \"))\n",
    "# \"elephant a noel\"\n",
    "\n",
    "print(tokenisation(normalize_query(\"  Éléphant   à Noël  \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf13e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_in_brand_index(tokens: list):\n",
    "\n",
    "    for token in tokens: \n",
    "        if token in brand_index.keys():\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_in_description_index(tokens: list):\n",
    "\n",
    "    for token in tokens: \n",
    "        if token in description_index.keys():\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48ef2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_in_origin_index(tokens: list):\n",
    "\n",
    "    for token in tokens: \n",
    "        if token in origin_index.keys():\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "151f1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_in_title_index(tokens: list):\n",
    "\n",
    "    for token in tokens: \n",
    "        if token in title_index.keys():\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a1560",
   "metadata": {},
   "source": [
    "## 3. Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c2e0a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(corpus: list, index):\n",
    "\n",
    "    words_index = []\n",
    "\n",
    "    for word in index.keys():\n",
    "        words_index.append(word)\n",
    "\n",
    "    corpus.append(words_index)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c383130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_index = [\n",
    "    brand_index,\n",
    "    description_index,\n",
    "    origin_index,\n",
    "    origin_synonyms,\n",
    "    reviews_index,\n",
    "    title_index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8408a30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de428c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for index in list_index:\n",
    "    create_corpus(corpus=corpus, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0ca026df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.2901406  2.08205007 0.         0.         0.68664027]\n"
     ]
    }
   ],
   "source": [
    "import rank_bm25\n",
    "\n",
    "\n",
    "bm25 = rank_bm25.BM25Okapi(corpus=corpus)\n",
    "query = \"chocolate italy\".split()\n",
    "results = bm25.get_scores(query=query)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f9ca3523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a99dbd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(brand_index.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
